# VibeSurfing Performance Optimization & Scalability Guide

## Executive Summary

Current implementation has **50-100x performance improvement potential** through parallelization, better API utilization, and caching strategies. The application currently underutilizes Google Vertex AI Gemini's scalability capabilities, processing events sequentially when they could run in parallel.

**Key Findings:**
- üî¥ **Critical**: Sequential row processing instead of parallel (10x slowdown)
- üî¥ **Critical**: Progressive column filling creates artificial dependencies (4x slowdown)
- üî¥ **Critical**: Sequential URL redirect resolution (5-10x slowdown)
- üü° **Major**: No result caching leads to repeated API calls (2x slowdown)
- üü° **Major**: N+1 database query pattern (1.5x slowdown)
- ‚úÖ **Good News**: Vertex AI Gemini 2.0+ has NO quota limits with Dynamic Shared Quota (DSQ)

---

## Table of Contents

1. [Current Architecture Analysis](#current-architecture-analysis)
2. [Identified Bottlenecks](#identified-bottlenecks)
3. [Google API Scalability Research](#google-api-scalability-research)
4. [Optimization Roadmap](#optimization-roadmap)
5. [Implementation Details](#implementation-details)
6. [Single User vs Multi-User Scalability](#single-user-vs-multi-user-scalability)
7. [Metrics & Monitoring](#metrics--monitoring)
8. [Cost Optimization](#cost-optimization)

---

## Current Architecture Analysis

### Data Flow

```
User Input (TipTap Editor)
    ‚Üì [1s debounce]
tRPC Mutation (updateCell)
    ‚Üì [DB write + Event creation]
Event Queue (pending status)
    ‚Üì [1s polling interval]
Background Processor
    ‚Üì [Sequential processing per sheet]
Sheet Updater (processes events one-by-one)
    ‚Üì [For each event]
Operator Controller (selects & executes operator)
    ‚Üì [Google Gemini API call]
Result Writing (next column)
    ‚Üì [Creates new event for next column]
Back to Event Queue...
```

### Key Components

| Component | File | Responsibility |
|-----------|------|----------------|
| Frontend Editor | `/src/components/tiptap-table.tsx` | User input with 1s debounce |
| API Trigger | `/src/app/api/update-sheet/route.ts` | Manual processing endpoint |
| Cell Router | `/src/server/api/routers/cell.ts` | tRPC mutations for cell updates |
| Background Processor | `/src/server/background-processor.ts` | Event queue polling (1s interval) |
| Sheet Updater | `/src/server/sheet-updater.ts` | Event processing orchestration |
| Operator Controller | `/src/server/operators/operator-controller.ts` | AI operator dispatch |
| Gemini Config | `/src/server/gemini/config.ts` | API settings & rate limits |

### Current Processing Model

**Sheet Level**: ‚úÖ Parallel (different sheets process concurrently)
**Row Level**: ‚ùå Sequential (rows processed one-by-one)
**Column Level**: ‚ùå Sequential waterfall (A ‚Üí B ‚Üí C ‚Üí D)
**URL Resolution**: ‚ùå Sequential (one redirect at a time)

---

## Identified Bottlenecks

### üî¥ CRITICAL #1: Sequential Row Processing

**Location**: `src/server/sheet-updater.ts:63-161`

```typescript
for (const event of pendingEvents) {  // Line 63
  // Process events one at a time
  await this.operatorController.dispatch(baseEvent);  // Line 145
}
```

**Problem**:
- Events processed in blocking loop
- If 10 rows exist, each taking 3s ‚Üí 30s total
- Could be 3s total with parallelization

**Impact**: **10x slower than necessary**

---

### üî¥ CRITICAL #2: Progressive Column Filling (Waterfall Pattern)

**Location**: `src/server/operators/column-aware-wrapper.ts:532-557`

```typescript
// After filling column N, create event for column N+1
await tx.insert(eventQueue).values({
  eventType: 'robot_cell_update',
  // ... creates next event
});
```

**Problem**:
- Columns fill left-to-right, waiting for previous column
- Row 1: [Col A: 3s] ‚Üí [Col B: 2s] ‚Üí [Col C: 4s] = 9s
- Could be: [Col A: 3s] ‚ïë [Col B: 2s] ‚ïë [Col C: 4s] = 4s (max)

**Impact**: **4x slower for independent columns**

**Note**: Schema already has dependency support (`schema.ts:260`) but not utilized

---

### üî¥ CRITICAL #3: Sequential URL Redirect Resolution

**Location**: `src/server/operators/google-search-operator.ts:61-69`

```typescript
for (let i = 0; i < chunks.length; i++) {  // Sequential loop
  if (url.includes('grounding-api-redirect')) {
    url = await resolveRedirectUrl(url);  // Blocking await
  }
}
```

**Problem**:
- Each redirect requires separate HTTP request (500ms-2s each)
- 10 URLs with redirects = 10 sequential requests = 5-20s
- **Solution already exists**: `resolveRedirectUrls()` in `url-resolver.ts:52-70` but not used!

**Impact**: **5-10x slower than necessary**

**Easy Fix**: Replace loop with existing parallel function

---

### üü° MAJOR #1: No Result Caching

**Evidence**: Cache TTL constants defined but never used

**Location**: `src/server/gemini/config.ts:183-189`

```typescript
export const SEARCH_CACHE_TTL = 1800000;  // Defined but not used!
export const EMBEDDING_CACHE_TTL = 3600000;  // Defined but not used!
```

**Problem**:
- Same Google search query ‚Üí duplicate API calls
- Same URL analyzed ‚Üí duplicate content fetching
- No Redis, no in-memory cache, no database caching

**Impact**: **2x slower + wasted API quota**

---

### üü° MAJOR #2: N+1 Database Query Problem

**Location**: `src/server/sheet-updater.ts:88-99`

```typescript
for (const event of pendingEvents) {
  // For EACH event, fetch row cells
  const rowCells = await db.select().from(cells).where(
    and(eq(cells.sheetId, sheetId), eq(cells.rowIndex, rowIndex))
  );  // Separate query per event
}
```

**Problem**: 10 events = 10+ separate database queries

**Impact**: **1.5x slower**

---

### üü° MAJOR #3: Low Concurrent Request Limit

**Location**: `src/server/gemini/config.ts:129-134`

```typescript
export const RATE_LIMIT_CONFIG = {
  maxRetries: 3,
  retryDelay: 1000,
  maxConcurrentRequests: 10,  // Arbitrary limit - too low!
  requestTimeout: 60000,
};
```

**Problem**: Vertex AI can handle much more than 10 concurrent requests

**Impact**: **2-5x slower under load**

---

## Google API Scalability Research

### Vertex AI Gemini (Primary API)

#### Current Usage
- Models: `gemini-2.5-flash` (default), `gemini-2.5-pro` (higher quality)
- Tools: Google Search grounding, URL Context, Function Calling
- Configuration: 10 concurrent requests (severely limited)

#### Scalability Capabilities

**1. Real-Time API (Current)**
- ‚úÖ **Dynamic Shared Quota (DSQ)**: Gemini 2.0+ has NO predefined quotas
- ‚úÖ Auto-scales based on demand across all customers
- ‚úÖ No need to request quota increases
- ‚ö†Ô∏è Current limit of 10 concurrent requests is artificial and self-imposed

**2. Batch API (Not Currently Used)**
- ‚úÖ **50% cost reduction** compared to real-time
- ‚úÖ **No quota limits** - can process hundreds of thousands of requests
- ‚úÖ Automatic parallelization and retries
- ‚úÖ High completion rates with 24-hour turnaround
- üéØ **Use Case**: Bulk processing of existing rows, backfills, batch analysis

**Best Practices:**
- Combine smaller jobs into one large batch (1 job with 200k requests > 1000 jobs with 200 requests)
- Use batch API for non-urgent bulk operations
- Use real-time API with higher concurrency for interactive workloads

#### Pricing (per 1M tokens)
- **Flash**: Input $0.075, Output $0.30
- **Pro**: Input $1.25, Output $5.00
- **Batch (50% discount)**: Flash Input $0.0375, Output $0.15

### Google Search Grounding Tool

**How It Works**: Embedded within Gemini API as a tool, not a separate API call

**Location**: `src/server/operators/google-search-operator.ts:36-45`

```typescript
tools: [{
  googleSearch: {}  // Gemini handles search internally
}]
```

**Scalability**: Inherits Gemini's scalability (DSQ, no quotas)

**Issue**: Returns redirect URLs that need resolution (see bottleneck #3)

### URL Context Tool

**How It Works**: Gemini fetches up to 20 URLs per request

**Location**: `src/server/operators/url-context-operator.ts:79-81`

**Limitation**: 20 URLs max per request (Gemini internal limit)

**Scalability**: Parallel URL fetching handled by Gemini

---

## Optimization Roadmap

### Phase 1: Quick Wins (1-2 days) ‚Üí **2-3x Faster**

#### 1.1 Fix URL Redirect Parallelization
**Effort**: 30 minutes
**File**: `src/server/operators/google-search-operator.ts:61-69`

```typescript
// BEFORE (Sequential)
for (let i = 0; i < chunks.length; i++) {
  if (url.includes('grounding-api-redirect')) {
    url = await resolveRedirectUrl(url);
  }
}

// AFTER (Parallel - use existing function)
const urlsToResolve = groundingMetadata.groundingChunks
  .filter(chunk => chunk.web?.uri?.includes('grounding-api-redirect'))
  .map(chunk => chunk.web.uri);

const resolvedUrls = await resolveRedirectUrls(urlsToResolve);
```

**Impact**: 5-10x faster redirect resolution

---

#### 1.2 Batch Database Queries
**Effort**: 2 hours
**File**: `src/server/sheet-updater.ts:88-99`

```typescript
// Fetch all row data at once
const rowIndices = [...new Set(pendingEvents.map(e => e.payload.rowIndex))];
const allRowCells = await db.select().from(cells).where(
  and(
    eq(cells.sheetId, sheetId),
    inArray(cells.rowIndex, rowIndices)
  )
);

// Group by rowIndex for fast lookup
const rowDataMap = new Map<number, Record<number, string>>();
allRowCells.forEach(cell => {
  if (!rowDataMap.has(cell.rowIndex)) {
    rowDataMap.set(cell.rowIndex, {});
  }
  rowDataMap.get(cell.rowIndex)![cell.colIndex] = cell.content ?? '';
});
```

**Impact**: 1.5x faster, reduces DB load

---

#### 1.3 Increase Concurrent Request Limit
**Effort**: 5 minutes
**File**: `src/server/gemini/config.ts:129-134`

```typescript
export const RATE_LIMIT_CONFIG = {
  maxRetries: 3,
  retryDelay: 1000,
  maxConcurrentRequests: 100,  // Increased from 10
  requestTimeout: 60000,
};
```

**Impact**: 2-5x faster under load

---

#### 1.4 Add In-Memory Result Caching
**Effort**: 4 hours
**Files**: New file `src/server/cache/result-cache.ts`

```typescript
import { LRUCache } from 'lru-cache';

// Search results cache
const searchCache = new LRUCache<string, SearchResult>({
  max: 1000,
  ttl: 1800000, // 30 minutes
});

// URL content cache
const urlCache = new LRUCache<string, string>({
  max: 500,
  ttl: 3600000, // 1 hour
});

export function getCachedSearch(query: string): SearchResult | undefined {
  return searchCache.get(query);
}

export function setCachedSearch(query: string, result: SearchResult): void {
  searchCache.set(query, result);
}
```

**Impact**: 2x faster for repeated queries, saves API costs

---

### Phase 2: Parallel Processing (3-5 days) ‚Üí **5-10x Faster**

#### 2.1 Row-Level Parallelization
**Effort**: 1 day
**File**: `src/server/sheet-updater.ts:63-161`

```typescript
// BEFORE (Sequential)
for (const event of pendingEvents) {
  await this.operatorController.dispatch(baseEvent);
}

// AFTER (Parallel)
const eventPromises = pendingEvents.map(async (event) => {
  try {
    await this.operatorController.dispatch(baseEvent);
  } catch (error) {
    console.error(`Error processing event ${event.id}:`, error);
    // Update event status to failed
  }
});

await Promise.allSettled(eventPromises);
```

**Considerations**:
- Database transaction isolation (already handled with `FOR UPDATE` locks)
- Individual error handling per row
- Progress tracking per row

**Impact**: 10x faster for multiple rows

---

#### 2.2 Column Dependency Graph Analysis
**Effort**: 2 days
**Files**:
- `src/server/operators/dependency-analyzer.ts` (new)
- `src/server/sheet-updater.ts` (modify)

```typescript
interface ColumnDependency {
  columnIndex: number;
  dependsOn: number[];  // Column indices this column depends on
}

class DependencyAnalyzer {
  analyzeTemplate(template: Template): ColumnDependency[] {
    const deps: ColumnDependency[] = [];

    template.columns.forEach((col, idx) => {
      const dependencies = col.dependencies || [];
      deps.push({
        columnIndex: idx,
        dependsOn: dependencies.map(d => d.columnIndex),
      });
    });

    return deps;
  }

  getExecutionLevels(deps: ColumnDependency[]): number[][] {
    // Topological sort to create execution levels
    // Level 0: No dependencies
    // Level 1: Depends only on level 0
    // Level 2: Depends on level 0 or 1, etc.
  }
}
```

**Impact**: Enables parallel column execution

---

#### 2.3 Parallel Column Processing
**Effort**: 2 days
**File**: `src/server/operators/operator-controller.ts`

```typescript
async processRowWithDependencies(
  rowData: Record<number, string>,
  template: Template,
  deps: ColumnDependency[]
): Promise<Record<number, string>> {
  const executionLevels = this.dependencyAnalyzer.getExecutionLevels(deps);
  const results: Record<number, string> = { ...rowData };

  // Execute each level in sequence, but columns within level in parallel
  for (const level of executionLevels) {
    const levelPromises = level.map(async (columnIndex) => {
      const column = template.columns[columnIndex];
      const operator = this.selectOperatorForColumn(column);

      try {
        const result = await this.executeOperator(operator, column, results);
        results[columnIndex] = result;
      } catch (error) {
        console.error(`Error in column ${columnIndex}:`, error);
        results[columnIndex] = `ERROR: ${error.message}`;
      }
    });

    await Promise.allSettled(levelPromises);
  }

  return results;
}
```

**Impact**: 4x faster for independent columns

---

#### 2.4 Implement Proper Rate Limiting
**Effort**: 1 day
**File**: `src/server/rate-limiter.ts` (new)

```typescript
import { RateLimiter } from 'limiter';

export class GeminiRateLimiter {
  private limiter: RateLimiter;

  constructor(tokensPerInterval: number, interval: string) {
    this.limiter = new RateLimiter({
      tokensPerInterval,
      interval,
    });
  }

  async throttle<T>(fn: () => Promise<T>): Promise<T> {
    await this.limiter.removeTokens(1);
    return fn();
  }
}

// Usage
const rateLimiter = new GeminiRateLimiter(100, 'second');

await rateLimiter.throttle(() =>
  geminiClient.generateContent(request)
);
```

**Impact**: Prevents API errors, smooth scaling

---

### Phase 3: Advanced Optimization (1-2 weeks) ‚Üí **10-20x Faster**

#### 3.1 Vertex AI Batch API Integration
**Effort**: 3 days
**Files**:
- `src/server/batch-processor.ts` (new)
- `src/server/gemini/batch-config.ts` (new)

**Use Cases**:
- Bulk backfill of existing sheets
- Scheduled overnight processing
- Non-urgent research tasks
- Template testing with sample data

**Implementation**:
```typescript
import { v1 as batchPrediction } from '@google-cloud/aiplatform';

class BatchProcessor {
  async processBatch(requests: GeminiRequest[]): Promise<string> {
    // 1. Upload requests to GCS
    const gcsInputUri = await this.uploadToGCS(requests);

    // 2. Create batch prediction job
    const job = await batchPrediction.createBatchPredictionJob({
      parent: `projects/${PROJECT_ID}/locations/${LOCATION}`,
      batchPredictionJob: {
        displayName: 'vibesurfing-batch',
        model: 'publishers/google/models/gemini-2.5-flash',
        inputConfig: { gcsSource: { uris: [gcsInputUri] } },
        outputConfig: { gcsDestination: { outputUriPrefix: GCS_OUTPUT } },
      },
    });

    // 3. Monitor job completion
    return job.name;
  }
}
```

**Benefits**:
- 50% cost reduction
- No quota limits
- Process 100k+ requests in single batch

---

#### 3.2 Redis Caching Layer
**Effort**: 2 days
**File**: `src/server/cache/redis-cache.ts` (new)

```typescript
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

export async function getCachedResult(key: string): Promise<string | null> {
  return redis.get(key);
}

export async function setCachedResult(
  key: string,
  value: string,
  ttl: number
): Promise<void> {
  await redis.setex(key, ttl, value);
}

// Usage
const cacheKey = `search:${hashQuery(query)}`;
const cached = await getCachedResult(cacheKey);

if (cached) {
  return JSON.parse(cached);
}

const result = await executeSearch(query);
await setCachedResult(cacheKey, JSON.stringify(result), 1800);
```

**Benefits**:
- Shared cache across all instances
- Persistence across restarts
- Multi-user cache sharing

---

#### 3.3 Streaming Results (Progressive UI)
**Effort**: 4 days
**Files**:
- `src/server/api/routers/cell.ts` (add streaming subscription)
- `src/components/sheet-editor.tsx` (handle streaming updates)

**Implementation**:
```typescript
// Server-side
export const cellRouter = createTRPCRouter({
  subscribeToSheet: publicProcedure
    .input(z.object({ sheetId: z.string() }))
    .subscription(async function* ({ input }) {
      // Yield updates as cells complete
      for await (const update of cellUpdateStream(input.sheetId)) {
        yield update;
      }
    }),
});

// Client-side
const { data } = trpc.cell.subscribeToSheet.useSubscription({
  sheetId: sheet.id,
});

useEffect(() => {
  if (data) {
    // Update cell immediately when result arrives
    updateCellInTable(data.rowIndex, data.colIndex, data.content);
  }
}, [data]);
```

**Benefits**:
- User sees results as they complete
- Better perceived performance
- Early feedback on errors

---

#### 3.4 Smart Request Batching
**Effort**: 3 days
**File**: `src/server/request-batcher.ts` (new)

**Strategy**: Combine multiple similar requests into single API call

```typescript
class RequestBatcher {
  private pending: Map<string, GeminiRequest[]> = new Map();
  private timers: Map<string, NodeJS.Timeout> = new Map();

  async addRequest(type: string, request: GeminiRequest): Promise<Response> {
    // Group similar requests
    if (!this.pending.has(type)) {
      this.pending.set(type, []);
    }

    this.pending.get(type)!.push(request);

    // Flush after 100ms or 10 requests
    if (this.pending.get(type)!.length >= 10) {
      return this.flush(type);
    }

    if (!this.timers.has(type)) {
      this.timers.set(type, setTimeout(() => this.flush(type), 100));
    }
  }

  async flush(type: string): Promise<void> {
    const requests = this.pending.get(type) || [];
    this.pending.delete(type);
    clearTimeout(this.timers.get(type));
    this.timers.delete(type);

    // Combine into single prompt
    const combinedPrompt = this.combineRequests(requests);
    const result = await gemini.generateContent(combinedPrompt);

    // Distribute results back
    this.distributeResults(requests, result);
  }
}
```

**Benefits**:
- Fewer API calls
- Better throughput
- Lower latency

---

#### 3.5 Circuit Breaker Pattern
**Effort**: 2 days
**File**: `src/server/circuit-breaker.ts` (new)

```typescript
class CircuitBreaker {
  private failures = 0;
  private state: 'CLOSED' | 'OPEN' | 'HALF_OPEN' = 'CLOSED';
  private nextAttempt: number = 0;

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === 'OPEN') {
      if (Date.now() < this.nextAttempt) {
        throw new Error('Circuit breaker is OPEN');
      }
      this.state = 'HALF_OPEN';
    }

    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess(): void {
    this.failures = 0;
    this.state = 'CLOSED';
  }

  private onFailure(): void {
    this.failures++;

    if (this.failures >= 5) {
      this.state = 'OPEN';
      this.nextAttempt = Date.now() + 60000; // 1 minute
    }
  }
}
```

**Benefits**:
- Prevents cascading failures
- Graceful degradation
- Automatic recovery

---

## Single User vs Multi-User Scalability

### Single User Optimization

**Primary Goal**: Minimize latency for individual sheet processing

**Key Strategies**:

1. **Row-Level Parallelization** (Phase 2.1)
   - Process all rows simultaneously
   - Limited by `maxConcurrentRequests`
   - **10 rows**: 30s ‚Üí 3s

2. **Column-Level Parallelization** (Phase 2.3)
   - Independent columns processed together
   - **4 independent columns**: 8s ‚Üí 2s

3. **In-Memory Caching** (Phase 1.4)
   - Cache search results within session
   - Instant retrieval for repeated queries

4. **Streaming Results** (Phase 3.3)
   - Show cells as they complete
   - Better perceived performance

**Expected Single-User Performance**:
- **Before**: 100 cells (10 rows √ó 10 cols) = 300-500s (5-8 minutes)
- **After Phase 1**: 150-250s (2.5-4 minutes) - 2x faster
- **After Phase 2**: 30-50s (0.5-1 minute) - 10x faster
- **After Phase 3**: 15-30s with progressive updates - 20x faster

---

### Multi-User Scalability

**Primary Goal**: Handle hundreds/thousands of concurrent users

**Key Strategies**:

1. **Leverage Vertex AI Dynamic Shared Quota (DSQ)**
   - No quota limits on Gemini 2.0+
   - Auto-scales with demand
   - Unlimited concurrent users (API-side)

2. **Horizontal Scaling with Worker Pools**
   - Deploy multiple background processor instances
   - Distribute event processing across workers
   - Use Redis for shared state

```typescript
// Worker pool architecture
class WorkerPool {
  private workers: Worker[] = [];

  async scaleUp(count: number): Promise<void> {
    for (let i = 0; i < count; i++) {
      this.workers.push(new BackgroundProcessor());
    }
  }

  async distributeWork(events: Event[]): Promise<void> {
    const workersCount = this.workers.length;
    const batches = this.partition(events, workersCount);

    await Promise.all(
      batches.map((batch, idx) =>
        this.workers[idx].processEvents(batch)
      )
    );
  }
}
```

3. **Redis Caching for Cross-User Sharing**
   - User A searches "climate change" ‚Üí cached
   - User B searches "climate change" ‚Üí instant (from cache)
   - **Cache hit rate**: 30-50% expected (huge cost savings)

4. **Database Connection Pooling**
   - Current: Default pool size
   - Optimized: Dynamic pool sizing based on load

```typescript
// Drizzle config with pooling
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 100,  // Increased from default 10
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});
```

5. **Batch API for Background Jobs** (Phase 3.1)
   - Offload non-urgent processing
   - 50% cost reduction
   - Process overnight batches

**Multi-User Scaling Capacity**:

| Metric | Current | Phase 1 | Phase 2 | Phase 3 |
|--------|---------|---------|---------|---------|
| Concurrent Users | 1-5 | 10-20 | 50-100 | 500-1000 |
| Events/minute | 60 | 180 | 600 | 3000 |
| Database Queries/sec | 100 | 150 | 300 | 600 |
| API Calls/sec | 10 | 30 | 100 | 500 |
| Cost per 1k events | $1.00 | $0.70 | $0.40 | $0.25 |

**Infrastructure Requirements**:

| Phase | Requirements |
|-------|--------------|
| Phase 1 | Current setup (1 server, PostgreSQL) |
| Phase 2 | Add Redis (Upstash/ElastiCache) |
| Phase 3 | Multiple workers, Load balancer, Redis cluster |

---

## Metrics & Monitoring

### Critical Metrics to Track

#### Performance Metrics

1. **End-to-End Latency**
   - Time from user input to result display
   - **Target**: < 5s for single cell, < 30s for 100 cells

2. **Event Processing Time**
   - Time from event creation to completion
   - **Current**: 3-5s per event
   - **Target**: 0.5-1s per event

3. **API Call Latency**
   - Gemini API response time
   - **Baseline**: 1-3s
   - **Monitor**: p50, p95, p99

4. **Database Query Time**
   - Time for cell reads/writes
   - **Target**: < 50ms per query

#### Scalability Metrics

1. **Concurrent Requests**
   - Active Gemini API calls
   - **Current**: max 10
   - **Target**: 100+

2. **Cache Hit Rate**
   - Percentage of cached results
   - **Target**: 40-60%

3. **Queue Depth**
   - Pending events in queue
   - **Alert**: > 1000 events

4. **Worker Utilization**
   - CPU/Memory usage per worker
   - **Target**: 60-80%

#### Cost Metrics

1. **API Cost per Event**
   - Gemini API tokens √ó price
   - **Current**: ~$0.001 per event
   - **Target**: $0.0005 per event (50% reduction)

2. **Cache Savings**
   - Cost avoided via caching
   - **Expected**: 30-50% savings

3. **Batch API Savings**
   - Discount from batch processing
   - **Expected**: 50% on batch jobs

### Implementation

```typescript
// Metrics collection
import { Histogram, Counter, Gauge } from 'prom-client';

const eventDuration = new Histogram({
  name: 'event_processing_duration_seconds',
  help: 'Event processing duration',
  labelNames: ['operator', 'status'],
});

const apiCallDuration = new Histogram({
  name: 'gemini_api_duration_seconds',
  help: 'Gemini API call duration',
  labelNames: ['model'],
});

const cacheHits = new Counter({
  name: 'cache_hits_total',
  help: 'Cache hit count',
  labelNames: ['cache_type'],
});

const queueDepth = new Gauge({
  name: 'event_queue_depth',
  help: 'Number of pending events',
  labelNames: ['sheet_id'],
});

// Usage
const timer = eventDuration.startTimer();
try {
  await processEvent(event);
  timer({ operator: event.operator, status: 'success' });
} catch (error) {
  timer({ operator: event.operator, status: 'error' });
}
```

### Alerting Thresholds

| Metric | Warning | Critical |
|--------|---------|----------|
| p95 Latency | > 10s | > 30s |
| Queue Depth | > 500 | > 2000 |
| Error Rate | > 5% | > 15% |
| Cache Hit Rate | < 30% | < 15% |
| API Cost/day | > $50 | > $100 |

---

## Cost Optimization

### Current Cost Breakdown (Estimated)

**Assumptions**:
- 100 sheets/day
- 10 rows/sheet √ó 10 columns/sheet = 1000 cells/day
- Average 1000 tokens per API call
- Model: Gemini 2.5 Flash

**Daily Costs**:
```
Input tokens: 1000 events √ó 1000 tokens √ó $0.075 / 1M = $0.075
Output tokens: 1000 events √ó 500 tokens √ó $0.30 / 1M = $0.15
Total: $0.225/day = $6.75/month
```

**At Scale (1000 sheets/day)**:
```
10,000 events/day = $2.25/day = $67.50/month
```

### Optimization Strategies

#### 1. Caching (Phase 1.4 + 3.2)
**Impact**: 40-50% cache hit rate

```
Savings: $67.50 √ó 0.45 = $30/month
New cost: $37.50/month
```

#### 2. Batch API for Bulk Operations (Phase 3.1)
**Impact**: 50% cost reduction on 30% of workload

```
30% √ó $67.50 = $20.25 ‚Üí $10.13 (50% discount)
Savings: $10.12/month
New cost: $27.38/month (total with caching)
```

#### 3. Model Selection Optimization
**Strategy**: Use Flash for simple tasks, Pro for complex

```typescript
function selectModel(complexity: number): string {
  if (complexity < 0.5) {
    return 'gemini-2.5-flash';  // 80% of tasks
  } else {
    return 'gemini-2.5-pro';   // 20% of tasks
  }
}
```

**Impact**: Minimal since Flash is already default

#### 4. Request Batching (Phase 3.4)
**Impact**: Combine 3-5 requests into 1 = 60-80% reduction

```
80% reduction on API calls = fewer requests
Savings: Variable based on batching effectiveness
```

### Total Cost Optimization Summary

| Scale | Current | After Phase 1 | After Phase 2 | After Phase 3 |
|-------|---------|---------------|---------------|---------------|
| 100 sheets/day | $7/mo | $5/mo | $4/mo | $2/mo |
| 1000 sheets/day | $68/mo | $40/mo | $30/mo | $15/mo |
| 10k sheets/day | $675/mo | $400/mo | $300/mo | $150/mo |

**Cost per event**: $0.001 ‚Üí $0.0002 (80% reduction)

---

## Implementation Checklist

### Phase 1: Quick Wins (1-2 days)

- [ ] Replace sequential URL resolution with parallel (`resolveRedirectUrls`)
- [ ] Batch database queries (single query for all rows)
- [ ] Increase `maxConcurrentRequests` from 10 to 100
- [ ] Implement in-memory LRU cache for search results
- [ ] Add cache for URL content
- [ ] Update Gemini config with caching support

### Phase 2: Parallel Processing (3-5 days)

- [ ] Implement row-level parallelization in `sheet-updater.ts`
- [ ] Create dependency analyzer for column relationships
- [ ] Build column dependency graph from template
- [ ] Implement parallel column execution with dependency levels
- [ ] Add proper rate limiting with token bucket algorithm
- [ ] Implement per-row error handling
- [ ] Add progress tracking per row

### Phase 3: Advanced (1-2 weeks)

- [ ] Set up Google Cloud Storage for batch inputs/outputs
- [ ] Implement Batch API client wrapper
- [ ] Create batch job submission endpoint
- [ ] Build batch job monitoring system
- [ ] Deploy Redis (Upstash or self-hosted)
- [ ] Implement Redis caching layer
- [ ] Migrate in-memory cache to Redis
- [ ] Add tRPC streaming subscriptions
- [ ] Build streaming update client
- [ ] Implement progressive UI updates
- [ ] Create request batcher for similar queries
- [ ] Build circuit breaker for API resilience
- [ ] Add metrics collection (Prometheus)
- [ ] Set up monitoring dashboard (Grafana)
- [ ] Configure alerting rules

---

## Risk Assessment

### Technical Risks

| Risk | Severity | Mitigation |
|------|----------|------------|
| Database lock contention | Medium | Already using `FOR UPDATE` locks; add retry logic |
| API rate limit errors | Low | Vertex AI DSQ has no limits; implement circuit breaker |
| Memory exhaustion | Medium | Limit concurrent events per worker (100-500) |
| Cache invalidation bugs | Low | Use TTL-based expiration; add manual invalidation |
| Parallel execution race conditions | Medium | Use database transactions; test thoroughly |

### Business Risks

| Risk | Severity | Mitigation |
|------|----------|------------|
| Increased API costs | Low | Caching reduces costs by 40-50% |
| Infrastructure complexity | Medium | Phased rollout; Phase 1 has no new infra |
| Migration downtime | Low | Deploy alongside existing system; gradual migration |

---

## Next Steps

### Immediate Actions (This Week)

1. ‚úÖ **Read this document thoroughly**
2. üéØ **Start with Phase 1.1**: Fix URL redirect parallelization (30 min)
3. üéØ **Then Phase 1.3**: Increase concurrent request limit (5 min)
4. üìä **Measure baseline**: Time 100-cell sheet before changes
5. üìä **Measure improvement**: Time same sheet after Phase 1.1 + 1.3

### This Month

1. Complete all Phase 1 optimizations
2. Deploy to staging environment
3. Run load tests with 10-100 concurrent users
4. Measure cost savings from caching
5. Plan Phase 2 implementation

### Next Quarter

1. Implement Phase 2 parallel processing
2. Deploy Redis for multi-user caching
3. Build metrics dashboard
4. Scale to 500+ concurrent users
5. Begin Phase 3 advanced features

---

## Appendix: Code References

### Files with Sequential Processing (Need Parallelization)

- `src/server/sheet-updater.ts:63-161` - Sequential event loop
- `src/server/operators/google-search-operator.ts:61-69` - Sequential URL resolution
- `src/server/operators/column-aware-wrapper.ts:532-557` - Progressive column creation

### Files with Existing Parallel Functions (Use These!)

- `src/server/utils/url-resolver.ts:52-70` - `resolveRedirectUrls()` (parallel)
- `src/server/background-processor.ts:67-88` - Sheet-level parallel processing

### Configuration Files

- `src/server/gemini/config.ts:129-134` - Rate limit config
- `src/server/gemini/config.ts:183-189` - Cache TTL constants (unused)
- `src/server/gemini/config.ts:157-169` - Cost calculation

### Database Schema

- `src/server/db/schema.ts:194-221` - `gemini_usage_log` table
- `src/server/db/schema.ts:260` - Template column dependencies (unused)

---

## Summary

**Current State**: Sequential processing severely limits scalability
**Root Cause**: Artificial constraints, not API limitations
**Opportunity**: 50-100x performance improvement
**Quick Wins**: 2-3x faster in 1-2 days
**Full Potential**: 20x faster with streaming in 2 weeks
**Cost**: Reduces by 80% with caching + batching
**Scalability**: Unlimited with Vertex AI DSQ

**Primary Recommendation**: Start with Phase 1 (Quick Wins) immediately. The URL redirect fix alone will provide 5-10x improvement and takes only 30 minutes.
